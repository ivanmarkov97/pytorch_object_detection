{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "instance_segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y66ZDLoFOzax"
      },
      "source": [
        "import os\r\n",
        "import copy\r\n",
        "import time\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import functools\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as opt\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.utils.data as data\r\n",
        "\r\n",
        "import torchvision\r\n",
        "import torchvision.models as models\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "SEED = 241"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePopoHId95qR"
      },
      "source": [
        "def seed_everything(seed):\r\n",
        "  random.seed(seed)\r\n",
        "  np.random.seed(seed)\r\n",
        "  torch.manual_seed(seed)\r\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "\r\n",
        "  if torch.cuda.is_available(): \r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed_all(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    torch.backends.cudnn.benchmark = True\r\n",
        "\r\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2lnzIo97Dh",
        "outputId": "dc78bd61-0119-401f-c94f-b8f3ef4267d0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "if os.getcwd() != '/content/drive/My Drive':\r\n",
        "  os.chdir(os.getcwd() + '/drive/MyDrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E1d8VRyHfmz",
        "outputId": "f3bbb050-9e73-4032-9243-d26521bfa88f"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "DIR=\"PennFudanPed\"\r\n",
        "if [-d \"$DIR\"]; then\r\n",
        "  echo \"Installing in ${DIR}\"\r\n",
        "  unzip PennFudanPed.zip -d \"${DIR}\"\r\n",
        "else\r\n",
        "  echo \"Create and installing in ${DIR}\"\r\n",
        "  mkdir \"${DIR}\"\r\n",
        "  unzip PennFudanPed.zip -d \"${DIR}\"\r\n",
        "fi"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 2: [-d: command not found\n",
            "Create and installing in PennFudanPed\n",
            "mkdir: cannot create directory ‘PennFudanPed’: File exists\n",
            "Archive:  PennFudanPed.zip\n",
            "replace PennFudanPed/PennFudanPed/added-object-list.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/Annotation/FudanPed00001.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/Annotation/FudanPed00002.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/Annotation/FudanPed00003.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/PedMasks/FudanPed00001_mask.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/PedMasks/FudanPed00002_mask.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/PedMasks/FudanPed00003_mask.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/PNGImages/FudanPed00001.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/PNGImages/FudanPed00002.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/PNGImages/FudanPed00003.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace PennFudanPed/PennFudanPed/readme.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGEuZ5BpKIJ7"
      },
      "source": [
        "class PennFudanPedDataset(data.Dataset):\r\n",
        "\r\n",
        "  def __init__(self, path):\r\n",
        "    self.image_path = path + '/PNGImages/'\r\n",
        "    self.mask_path = path + '/PedMasks/'\r\n",
        "    self.images = os.listdir(os.getcwd() + self.image_path)\r\n",
        "    self.masks = os.listdir(os.getcwd() + self.mask_path)\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    image_path = os.getcwd() + self.image_path + self.images[idx]\r\n",
        "    mask_path = os.getcwd() + self.mask_path + self.masks[idx]\r\n",
        "\r\n",
        "    image = np.array(Image.open(image_path).convert('RGB'))\r\n",
        "    mask = np.array(Image.open(mask_path))\r\n",
        "\r\n",
        "    # instances are encoded as different colors\r\n",
        "    obj_ids = np.unique(mask)\r\n",
        "    # first id is the background, so remove it\r\n",
        "    obj_ids = obj_ids[1:]\r\n",
        "\r\n",
        "    # split the color-encoded mask into a set\r\n",
        "    # of binary masks\r\n",
        "    masks = mask == obj_ids[:, None, None]\r\n",
        "\r\n",
        "    # get bounding box coordinates for each mask\r\n",
        "    num_objs = len(obj_ids)\r\n",
        "    boxes = []\r\n",
        "    for i in range(num_objs):\r\n",
        "        pos = np.where(masks[i])\r\n",
        "        xmin = np.min(pos[1])\r\n",
        "        xmax = np.max(pos[1])\r\n",
        "        ymin = np.min(pos[0])\r\n",
        "        ymax = np.max(pos[0])\r\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\r\n",
        "\r\n",
        "    # convert everything into a torch.Tensor\r\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\r\n",
        "    # there is only one class\r\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\r\n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\r\n",
        "\r\n",
        "    image_id = torch.tensor([idx])\r\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\r\n",
        "    # suppose all instances are not crowd\r\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\r\n",
        "\r\n",
        "    target = {}\r\n",
        "    target[\"boxes\"] = boxes\r\n",
        "    target[\"labels\"] = labels\r\n",
        "    target[\"masks\"] = masks\r\n",
        "    target[\"image_id\"] = image_id\r\n",
        "    target[\"area\"] = area\r\n",
        "    target[\"iscrowd\"] = iscrowd\r\n",
        "\r\n",
        "    return image, target\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.images)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10D87YAMWVb"
      },
      "source": [
        "batch_size = 1\r\n",
        "\r\n",
        "dataset = PennFudanPedDataset('/PennFudanPed/PennFudanPed')\r\n",
        "train_loader = data.DataLoader(dataset, batch_size=batch_size)\r\n",
        "# dataset[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WvRpR7dURVl"
      },
      "source": [
        "# From pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74MUxZhuVEiY"
      },
      "source": [
        "# print(model)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvD4fnGuTrbv"
      },
      "source": [
        "predictor = models.detection.faster_rcnn.FastRCNNPredictor\r\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n",
        "\r\n",
        "num_classes = 2\r\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\r\n",
        "model.roi_heads.box_predictor = predictor(in_features, num_classes)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4GOmV5PU04w"
      },
      "source": [
        "# print(model)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeD9Qi3xWOCL"
      },
      "source": [
        "# Modify backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhaAvK_qWP_E"
      },
      "source": [
        "backbone = models.mobilenet_v2(pretrained=True).features\r\n",
        "backbone.out_channels = 1280\r\n",
        "anchor_generator = models.detection.faster_rcnn.AnchorGenerator(sizes=(32, 64, 128, 256, 512), \r\n",
        "                                                                aspect_ratios=(0.5, 1.0, 2.0))\r\n",
        "\r\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\r\n",
        "                                                output_size=7,\r\n",
        "                                                sampling_ratio=2)\r\n",
        "\r\n",
        "model = models.detection.FasterRCNN(backbone, \r\n",
        "                                    num_classes=2,\r\n",
        "                                    rpn_anchor_generator=anchor_generator, \r\n",
        "                                    box_roi_pool=roi_pooler)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qTlL1uTZpMm"
      },
      "source": [
        "# Get pretrained instance segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeoa0jo9XYEE"
      },
      "source": [
        "import torchvision\r\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\r\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\r\n",
        "\r\n",
        "      \r\n",
        "def get_instance_segmentation_model(num_classes):\r\n",
        "    # load an instance segmentation model pre-trained on COCO\r\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\r\n",
        "\r\n",
        "    # get the number of input features for the classifier\r\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\r\n",
        "    # replace the pre-trained head with a new one\r\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\r\n",
        "\r\n",
        "    # now get the number of input features for the mask classifier\r\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\r\n",
        "    hidden_layer = 256\r\n",
        "    # and replace the mask predictor with a new one\r\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\r\n",
        "                                                       hidden_layer,\r\n",
        "                                                       num_classes)\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyA900yEgFt7"
      },
      "source": [
        "model = get_instance_segmentation_model(num_classes=2)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJA3Wio7avGH",
        "outputId": "718623ed-2c10-4989-839c-1a3c730004b4"
      },
      "source": [
        "device = torch.device('cpu')\r\n",
        "optimizer = opt.Adam(model.parameters(), lr=1e-5)\r\n",
        "\r\n",
        "model.train()\r\n",
        "\r\n",
        "for batch in train_loader:\r\n",
        "  optimizer.zero_grad()\r\n",
        "  images, targets = batch\r\n",
        "  images = list(image.permute(2, 0, 1).to(device) for image in images)\r\n",
        "  targets = [{k: v[0].to(device) for k, v in targets.items()}]\r\n",
        "  loss_dict = model(images, targets)\r\n",
        "  losses = sum(loss for loss in loss_dict.values())\r\n",
        "  losses.backward()\r\n",
        "  optimizer.step()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4])\n",
            "torch.Size([1, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahb2jSdPidkX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}